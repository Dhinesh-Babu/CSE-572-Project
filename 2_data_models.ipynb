{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation with All Models using Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import target.npy and train.npy\n",
    "# Train a model using train.npy\n",
    "# Test the model using target.npy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Import the data\n",
    "train = np.load('dataset/train.npy')\n",
    "target = np.load('dataset/target.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.9063245190622917\n",
      "RMSE: 1.8114579320050195\n",
      "RMSE: 1.856984817994204\n",
      "RMSE: 1.8469065603446553\n",
      "RMSE: 1.8518672416355286\n",
      "Average RMSE: 1.8547082142083398\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 5\n",
    "\n",
    "# Create an instance of KFold with the specified number of folds\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# Initialize an empty list to store the root mean squared errors (RMSE) for each fold\n",
    "rmse_scores = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, test_index in kf.split(train):\n",
    "    # Split the data into training and testing sets for the current fold\n",
    "    X_train, X_test = train[train_index], train[test_index]\n",
    "    y_train, y_test = target[train_index], target[test_index]\n",
    "    \n",
    "    # Create an instance of LinearRegression\n",
    "    model = LinearRegression()\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the testing data\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate the RMSE for the current fold\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    # Print the RMSE for the current fold\n",
    "    print(\"RMSE:\", rmse)\n",
    "    \n",
    "    # Append the RMSE to the list of scores\n",
    "    rmse_scores.append(rmse)\n",
    "\n",
    "# Calculate the average RMSE across all folds\n",
    "average_rmse = np.mean(rmse_scores)\n",
    "\n",
    "# Print the average RMSE\n",
    "print(\"Average RMSE:\", average_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost with KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.4121623\n",
      "RMSE: 1.3773588\n",
      "RMSE: 1.4391705\n",
      "RMSE: 1.4573854\n",
      "RMSE: 1.3527637\n",
      "Average RMSE: 1.4077681\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rmse_scores_xgboost = []\n",
    "\n",
    "for train_index, test_index in kf.split(train):\n",
    "    # Split the data into training and testing sets for the current fold\n",
    "    X_train, X_test = train[train_index], train[test_index]\n",
    "    y_train, y_test = target[train_index], target[test_index]\n",
    "    \n",
    "    # Create an instance of xgb.DMatrix for the training and testing sets\n",
    "    xgb_train = xgb.DMatrix(X_train, y_train)\n",
    "    xgb_test = xgb.DMatrix(X_test, y_test)\n",
    "\n",
    "    # Define the parameters for the XGBoost model\n",
    "    params = {\n",
    "        'colsample_bytree': 0.8,                 \n",
    "        'learning_rate': 0.1,\n",
    "        'max_depth': 5,\n",
    "        'subsample': 0.8,\n",
    "        'objective': 'reg:squarederror',\n",
    "    }\n",
    "\n",
    "    # Train the XGBoost model\n",
    "    model = xgb.train(params, xgb_train, num_boost_round=100)\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    y_pred = model.predict(xgb_test)\n",
    "\n",
    "    # Calculate the RMSE for the current fold\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    # Print the RMSE for the current fold\n",
    "    print(\"RMSE:\", rmse)\n",
    "\n",
    "    # Append the RMSE to the list of scores\n",
    "    rmse_scores_xgboost.append(rmse)\n",
    "\n",
    "\n",
    "# Calculate the average RMSE across all folds\n",
    "average_rmse = np.mean(rmse_scores_xgboost)\n",
    "\n",
    "# Print the average RMSE\n",
    "print(\"Average RMSE:\", average_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM with KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.4360894297773417\n",
      "RMSE: 1.3978521913589237\n",
      "RMSE: 1.4722209435886482\n",
      "RMSE: 1.4877702199875382\n",
      "RMSE: 1.3510469637861724\n",
      "Average RMSE: 1.4289959496997249\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rmse_scores_lightgbm = []\n",
    "\n",
    "for train_index, test_index in kf.split(train):\n",
    "    # Split the data into training and testing sets for the current fold\n",
    "    X_train, X_test = train[train_index], train[test_index]\n",
    "    y_train, y_test = target[train_index], target[test_index]\n",
    "    \n",
    "    # Create an instance of lgb.Dataset for the training and testing sets\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_test = lgb.Dataset(X_test, y_test)\n",
    "\n",
    "#     # Define the parameters for the LightGBM model\n",
    "#     params = {\n",
    "#     'objective': 'regression',\n",
    "#     'metric': 'rmse',\n",
    "#     'num_leaves': 100,\n",
    "#     'learning_rate': 0.09841079471843048,\n",
    "#     'feature_fraction': 0.6146295376710438,\n",
    "#     'bagging_fraction': 0.6360723189013848,\n",
    "#     'bagging_freq': 7,\n",
    "#     'verbose': 0\n",
    "# }\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': 0\n",
    "    }\n",
    "\n",
    "    # Train the LightGBM model\n",
    "    model = lgb.train(params, lgb_train, num_boost_round=100)\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate the RMSE for the current fold\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    # Print the RMSE for the current fold\n",
    "    print(\"RMSE:\", rmse)\n",
    "\n",
    "    # Append the RMSE to the list of scores\n",
    "    rmse_scores_lightgbm.append(rmse)\n",
    "\n",
    "\n",
    "# Calculate the average RMSE across all folds\n",
    "average_rmse = np.mean(rmse_scores_lightgbm)\n",
    "\n",
    "# Print the average RMSE\n",
    "print(\"Average RMSE:\", average_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression with k fold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge RMSE: 1.9063042351818684\n",
      "Ridge RMSE: 1.8112543823578529\n",
      "Ridge RMSE: 1.8569727958410456\n",
      "Ridge RMSE: 1.846936281576231\n",
      "Ridge RMSE: 1.8518775569484234\n",
      "Average Ridge RMSE: 1.8546690503810843\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 5\n",
    "\n",
    "# Create an instance of KFold with the specified number of folds\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# Initialize an empty list to store the RMSE for each fold\n",
    "ridge_rmse_scores = []\n",
    "\n",
    "# Specify the regularization strength for Ridge regression\n",
    "alpha_ridge = 1.0\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, test_index in kf.split(train):\n",
    "    # Split the data\n",
    "    X_train, X_test = train[train_index], train[test_index]\n",
    "    y_train, y_test = target[train_index], target[test_index]\n",
    "    \n",
    "    # Create an instance of Ridge regression\n",
    "    ridge_model = Ridge(alpha=alpha_ridge)\n",
    "    \n",
    "    # Fit the model\n",
    "    ridge_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = ridge_model.predict(X_test)\n",
    "    \n",
    "    # Calculate RMSE and append to the list\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(\"Ridge RMSE:\", rmse)\n",
    "    ridge_rmse_scores.append(rmse)\n",
    "\n",
    "# Calculate the average RMSE\n",
    "average_ridge_rmse = np.mean(ridge_rmse_scores)\n",
    "print(\"Average Ridge RMSE:\", average_ridge_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression with K-fold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso RMSE: 2.1332342216645404\n",
      "Lasso RMSE: 2.055400695762536\n",
      "Lasso RMSE: 2.0796900553177444\n",
      "Lasso RMSE: 2.0921591642184496\n",
      "Lasso RMSE: 2.080921801239417\n",
      "Average Lasso RMSE: 2.0882811876405376\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Initialize an empty list for Lasso RMSE scores\n",
    "lasso_rmse_scores = []\n",
    "\n",
    "# Specify the regularization strength for Lasso regression\n",
    "alpha_lasso = 1.0\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, test_index in kf.split(train):\n",
    "    # Split the data\n",
    "    X_train, X_test = train[train_index], train[test_index]\n",
    "    y_train, y_test = target[train_index], target[test_index]\n",
    "    \n",
    "    # Create an instance of Lasso regression\n",
    "    lasso_model = Lasso(alpha=alpha_lasso)\n",
    "    \n",
    "    # Fit the model\n",
    "    lasso_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = lasso_model.predict(X_test)\n",
    "    \n",
    "    # Calculate RMSE and append to the list\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(\"Lasso RMSE:\", rmse)\n",
    "    lasso_rmse_scores.append(rmse)\n",
    "\n",
    "# Calculate the average RMSE\n",
    "average_lasso_rmse = np.mean(lasso_rmse_scores)\n",
    "print(\"Average Lasso RMSE:\", average_lasso_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desicion tree with K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree RMSE: 1.345699841219152\n",
      "Decision Tree RMSE: 1.4083761384690157\n",
      "Decision Tree RMSE: 1.3400278448508927\n",
      "Decision Tree RMSE: 1.4701276760132027\n",
      "Decision Tree RMSE: 1.389838093577145\n",
      "Average Decision Tree RMSE: 1.3908139188258817\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 5\n",
    "\n",
    "# Create an instance of KFold with the specified number of folds\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# Initialize an empty list to store the RMSE for each fold\n",
    "dt_rmse_scores = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, test_index in kf.split(train):\n",
    "    # Split the data\n",
    "    X_train, X_test = train[train_index], train[test_index]\n",
    "    y_train, y_test = target[train_index], target[test_index]\n",
    "    \n",
    "    # Create an instance of DecisionTreeRegressor\n",
    "    # Adjust max_depth, min_samples_split, and/or min_samples_leaf as needed\n",
    "    dt_model = DecisionTreeRegressor(max_depth=None) # Use default values or adjust as necessary\n",
    "    \n",
    "    # Fit the model\n",
    "    dt_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = dt_model.predict(X_test)\n",
    "    \n",
    "    # Calculate RMSE and append to the list\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(\"Decision Tree RMSE:\", rmse)\n",
    "    dt_rmse_scores.append(rmse)\n",
    "\n",
    "# Calculate the average RMSE\n",
    "average_dt_rmse = np.mean(dt_rmse_scores)\n",
    "print(\"Average Decision Tree RMSE:\", average_dt_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest with k Fold -- TOO long to run. 30 min+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# import numpy as np\n",
    "\n",
    "# # Define the number of folds for cross-validation\n",
    "# n_splits = 5\n",
    "\n",
    "# # Create an instance of KFold with the specified number of folds\n",
    "# kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# # Initialize an empty list to store the RMSE for each fold\n",
    "# random_forest_rmse_scores = []\n",
    "\n",
    "# # Perform cross-validation\n",
    "# for train_index, test_index in kf.split(train):\n",
    "#     # Split the data into training and testing sets for the current fold\n",
    "#     X_train, X_test = train[train_index], train[test_index]\n",
    "#     y_train, y_test = target[train_index], target[test_index]\n",
    "    \n",
    "#     # Create an instance of the RandomForestRegressor\n",
    "#     random_forest_model = RandomForestRegressor(n_estimators=100)  # You can adjust the number of trees\n",
    "    \n",
    "#     # Fit the model to the training data\n",
    "#     random_forest_model.fit(X_train, y_train)\n",
    "    \n",
    "#     # Make predictions on the testing data\n",
    "#     y_pred = random_forest_model.predict(X_test)\n",
    "    \n",
    "#     # Calculate the RMSE for the current fold\n",
    "#     rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "#     # Print the RMSE for the current fold\n",
    "#     print(\"Random Forest RMSE:\", rmse)\n",
    "    \n",
    "#     # Append the RMSE to the list of scores\n",
    "#     random_forest_rmse_scores.append(rmse)# Calculate the average RMSE across all folds\n",
    "# average_random_forest_rmse = np.mean(random_forest_rmse_scores)\n",
    "\n",
    "# # Print the average RMSE\n",
    "# print(\"Average Random Forest RMSE:\", average_random_forest_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN RMSE: 1.8216407\n",
      "KNN RMSE: 1.806594\n",
      "KNN RMSE: 1.8060709\n",
      "KNN RMSE: 1.8160338\n",
      "KNN RMSE: 1.7953608\n",
      "Average KNN RMSE: 1.80914\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 5\n",
    "\n",
    "# Create an instance of KFold with the specified number of folds\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# Initialize an empty list to store the root mean squared errors (RMSE) for each fold\n",
    "rmse_scores_knn = []\n",
    "\n",
    "# Perform cross-validation for k-Nearest Neighbors (KNN)\n",
    "for train_index, test_index in kf.split(train):\n",
    "    # Split the data into training and testing sets for the current fold\n",
    "    X_train, X_test = train[train_index], train[test_index]\n",
    "    y_train, y_test = target[train_index], target[test_index]\n",
    "    \n",
    "    # Create an instance of k-Nearest Neighbors (KNN) Regressor\n",
    "    knn_model = KNeighborsRegressor(n_neighbors=5)  # You can change the number of neighbors as needed\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    knn_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the testing data\n",
    "    y_pred_knn = knn_model.predict(X_test)\n",
    "    \n",
    "    # Calculate the RMSE for the current fold for KNN\n",
    "    rmse_knn = np.sqrt(mean_squared_error(y_test, y_pred_knn))\n",
    "    \n",
    "    # Print the RMSE for the current fold for KNN\n",
    "    print(\"KNN RMSE:\", rmse_knn)\n",
    "    \n",
    "    # Append the RMSE to the list of scores for KNN\n",
    "    rmse_scores_knn.append(rmse_knn)\n",
    "\n",
    "# Calculate the average RMSE across all folds for KNN\n",
    "average_rmse_knn = np.mean(rmse_scores_knn)\n",
    "\n",
    "# Print the average RMSE for KNN\n",
    "print(\"Average KNN RMSE:\", average_rmse_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 2.0003244\ttest: 2.0472585\tbest: 2.0472585 (0)\ttotal: 2.44s\tremaining: 4m 1s\n",
      "99:\tlearn: 1.2659847\ttest: 1.3011109\tbest: 1.3011109 (99)\ttotal: 3m\tremaining: 0us\n",
      "\n",
      "bestTest = 1.301110861\n",
      "bestIteration = 99\n",
      "\n",
      "CatBoost RMSE: 1.3011108608183484\n",
      "0:\tlearn: 2.0152328\ttest: 1.9746628\tbest: 1.9746628 (0)\ttotal: 1.52s\tremaining: 2m 31s\n",
      "99:\tlearn: 1.2670117\ttest: 1.2884159\tbest: 1.2884159 (99)\ttotal: 3m 11s\tremaining: 0us\n",
      "\n",
      "bestTest = 1.288415886\n",
      "bestIteration = 99\n",
      "\n",
      "CatBoost RMSE: 1.2884158860782338\n",
      "0:\tlearn: 2.0067580\ttest: 2.0178050\tbest: 2.0178050 (0)\ttotal: 1.53s\tremaining: 2m 31s\n",
      "99:\tlearn: 1.2596054\ttest: 1.3373782\tbest: 1.3373782 (99)\ttotal: 3m 22s\tremaining: 0us\n",
      "\n",
      "bestTest = 1.337378205\n",
      "bestIteration = 99\n",
      "\n",
      "CatBoost RMSE: 1.3373782047030436\n",
      "0:\tlearn: 2.0040895\ttest: 2.0263057\tbest: 2.0263057 (0)\ttotal: 2.11s\tremaining: 3m 29s\n",
      "99:\tlearn: 1.2389128\ttest: 1.3779281\tbest: 1.3776739 (98)\ttotal: 3m 13s\tremaining: 0us\n",
      "\n",
      "bestTest = 1.377673873\n",
      "bestIteration = 98\n",
      "\n",
      "Shrink model to first 99 iterations.\n",
      "CatBoost RMSE: 1.3776738733695533\n",
      "0:\tlearn: 2.0127130\ttest: 1.9913227\tbest: 1.9913227 (0)\ttotal: 2.09s\tremaining: 3m 27s\n",
      "99:\tlearn: 1.2640655\ttest: 1.2421780\tbest: 1.2412302 (97)\ttotal: 2m 58s\tremaining: 0us\n",
      "\n",
      "bestTest = 1.241230215\n",
      "bestIteration = 97\n",
      "\n",
      "Shrink model to first 98 iterations.\n",
      "CatBoost RMSE: 1.2412302149332644\n",
      "Average CatBoost RMSE: 1.3091618079804885\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 5\n",
    "\n",
    "# Create an instance of KFold with the specified number of folds\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# Initialize an empty list to store the RMSE for each fold\n",
    "catboost_rmse_scores = []\n",
    "\n",
    "# Specify the parameters for the CatBoost model\n",
    "params = {\n",
    "    'iterations': 100,\n",
    "    'learning_rate': 0.1,\n",
    "    'depth': 10,\n",
    "    'loss_function': 'RMSE'\n",
    "}\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, test_index in kf.split(train):\n",
    "    # Split the data\n",
    "    X_train, X_test = train[train_index], train[test_index]\n",
    "    y_train, y_test = target[train_index], target[test_index]\n",
    "    \n",
    "    # Create an instance of CatBoostRegressor\n",
    "    model = CatBoostRegressor(**params)\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=100)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate RMSE and append to the list\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(\"CatBoost RMSE:\", rmse)\n",
    "    catboost_rmse_scores.append(rmse)\n",
    "\n",
    "# Calculate the average RMSE\n",
    "average_catboost_rmse = np.mean(catboost_rmse_scores)\n",
    "print(\"Average CatBoost RMSE:\", average_catboost_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dublu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m1966/7755\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 11ms/step - loss: 2.0661"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 35\u001b[0m\n\u001b[0;32m     31\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m1\u001b[39m  ,activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     33\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss \u001b[38;5;241m=\u001b[39m NN_RMSLE)\n\u001b[1;32m---> 35\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_xx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_yy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_xx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_yy\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mearlyStop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNeural Network Training RMSLE = \u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m.\u001b[39mevaluate(train, target, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m     37\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbasic_3layer.keras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:325\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    324\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 325\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(\n\u001b[0;32m    327\u001b[0m         step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    328\u001b[0m     )\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "from keras.models import load_model\n",
    "import tensorflow.keras.backend as K\n",
    "from graphviz import Digraph\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def NN_RMSLE(y_actual, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_actual))) \n",
    "\n",
    "train_xx, val_xx, train_yy, val_yy = train_test_split(train, target, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "earlyStop= EarlyStopping(monitor='val_loss', mode='min', patience=3)\n",
    "\n",
    "model.add(layers.Dense(288, activation='relu',input_shape=(train_xx.shape[1],)))\n",
    "model.add(Dropout(0))\n",
    "model.add(layers.Dense(224, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(layers.Dense(96, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(0  ,activation='linear'))\n",
    "\n",
    "model.compile(optimizer='adam', loss = NN_RMSLE)\n",
    "\n",
    "model.fit(train_xx, train_yy, epochs = 15, batch_size = 2048, validation_data=(val_xx,val_yy),callbacks = earlyStop)\n",
    "print('Neural Network Training RMSLE = ', model.evaluate(train, target, verbose=0))\n",
    "model.save('basic_3layer.keras')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_RMSLE(y_actual, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_actual))) \n",
    "model_nn = load_model('basic_3layer.keras',custom_objects={'NN_RMSLE' : NN_RMSLE })\n",
    "plot_model(model_nn, to_file='model.png', show_shapes=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
